{
    "type": "object",
    "properties": {
        "access_token": {
            "type": "string"
        },
        "alt": {
            "type": "string",
            "enum": [
                "json",
                "media",
                "proto"
            ],
            "default": "json"
        },
        "callback": {
            "type": "string"
        },
        "fields": {
            "type": "string"
        },
        "key": {
            "type": "string"
        },
        "oauth_token": {
            "type": "string"
        },
        "prettyPrint": {
            "type": "boolean",
            "default": true
        },
        "quotaUser": {
            "type": "string"
        },
        "uploadType": {
            "type": "string"
        },
        "upload_protocol": {
            "type": "string"
        },
        "requestBody": {
            "description": "The top-level message sent by the client for the `LongRunningRecognize`\nmethod.",
            "properties": {
                "audio": {
                    "description": "*Required* The audio data to be recognized.",
                    "properties": {
                        "content": {
                            "description": "The audio data bytes encoded as specified in\n`RecognitionConfig`. Note: as with all bytes fields, protobuffers use a\npure binary representation, whereas JSON representations use base64.",
                            "format": "byte",
                            "type": "string"
                        },
                        "uri": {
                            "description": "URI that points to a file that contains audio data bytes as specified in\n`RecognitionConfig`. The file must not be compressed (for example, gzip).\nCurrently, only Google Cloud Storage URIs are\nsupported, which must be specified in the following format:\n`gs://bucket_name/object_name` (other URI formats return\ngoogle.rpc.Code.INVALID_ARGUMENT). For more information, see\n[Request URIs](https://cloud.google.com/storage/docs/reference-uris).",
                            "type": "string"
                        }
                    },
                    "type": "object"
                },
                "config": {
                    "description": "*Required* Provides information to the recognizer that specifies how to\nprocess the request.",
                    "properties": {
                        "audioChannelCount": {
                            "description": "*Optional* The number of channels in the input audio data.\nONLY set this for MULTI-CHANNEL recognition.\nValid values for LINEAR16 and FLAC are `1`-`8`.\nValid values for OGG_OPUS are '1'-'254'.\nValid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.\nIf `0` or omitted, defaults to one channel (mono).\nNote: We only recognize the first channel by default.\nTo perform independent recognition on each channel set\n`enable_separate_recognition_per_channel` to 'true'.",
                            "format": "int32",
                            "type": "integer"
                        },
                        "enableAutomaticPunctuation": {
                            "description": "*Optional* If 'true', adds punctuation to recognition result hypotheses.\nThis feature is only available in select languages. Setting this for\nrequests in other languages has no effect at all.\nThe default 'false' value does not add punctuation to result hypotheses.\nNote: This is currently offered as an experimental service, complimentary\nto all users. In the future this may be exclusively available as a\npremium feature.",
                            "type": "boolean"
                        },
                        "enableSeparateRecognitionPerChannel": {
                            "description": "This needs to be set to `true` explicitly and `audio_channel_count` > 1\nto get each channel recognized separately. The recognition result will\ncontain a `channel_tag` field to state which channel that result belongs\nto. If this is not true, we will only recognize the first channel. The\nrequest is billed cumulatively for all channels recognized:\n`audio_channel_count` multiplied by the length of the audio.",
                            "type": "boolean"
                        },
                        "enableWordTimeOffsets": {
                            "description": "*Optional* If `true`, the top result includes a list of words and\nthe start and end time offsets (timestamps) for those words. If\n`false`, no word-level time offset information is returned. The default is\n`false`.",
                            "type": "boolean"
                        },
                        "encoding": {
                            "description": "Encoding of audio data sent in all `RecognitionAudio` messages.\nThis field is optional for `FLAC` and `WAV` audio files and required\nfor all other audio formats. For details, see AudioEncoding.",
                            "enum": [
                                "ENCODING_UNSPECIFIED",
                                "LINEAR16",
                                "FLAC",
                                "MULAW",
                                "AMR",
                                "AMR_WB",
                                "OGG_OPUS",
                                "SPEEX_WITH_HEADER_BYTE"
                            ],
                            "type": "string"
                        },
                        "languageCode": {
                            "description": "*Required* The language of the supplied audio as a\n[BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.\nExample: \"en-US\".\nSee [Language Support](/speech-to-text/docs/languages)\nfor a list of the currently supported language codes.",
                            "type": "string"
                        },
                        "maxAlternatives": {
                            "description": "*Optional* Maximum number of recognition hypotheses to be returned.\nSpecifically, the maximum number of `SpeechRecognitionAlternative` messages\nwithin each `SpeechRecognitionResult`.\nThe server may return fewer than `max_alternatives`.\nValid values are `0`-`30`. A value of `0` or `1` will return a maximum of\none. If omitted, will return a maximum of one.",
                            "format": "int32",
                            "type": "integer"
                        },
                        "metadata": {
                            "description": "*Optional* Metadata regarding this request.",
                            "properties": {
                                "audioTopic": {
                                    "description": "Description of the content. Eg. \"Recordings of federal supreme court\nhearings from 2012\".",
                                    "type": "string"
                                },
                                "industryNaicsCodeOfAudio": {
                                    "description": "The industry vertical to which this speech recognition request most\nclosely applies. This is most indicative of the topics contained\nin the audio.  Use the 6-digit NAICS code to identify the industry\nvertical - see https://www.naics.com/search/.",
                                    "format": "uint32",
                                    "type": "integer"
                                },
                                "interactionType": {
                                    "description": "The use case most closely describing the audio content to be recognized.",
                                    "enum": [
                                        "INTERACTION_TYPE_UNSPECIFIED",
                                        "DISCUSSION",
                                        "PRESENTATION",
                                        "PHONE_CALL",
                                        "VOICEMAIL",
                                        "PROFESSIONALLY_PRODUCED",
                                        "VOICE_SEARCH",
                                        "VOICE_COMMAND",
                                        "DICTATION"
                                    ],
                                    "type": "string"
                                },
                                "microphoneDistance": {
                                    "description": "The audio type that most closely describes the audio being recognized.",
                                    "enum": [
                                        "MICROPHONE_DISTANCE_UNSPECIFIED",
                                        "NEARFIELD",
                                        "MIDFIELD",
                                        "FARFIELD"
                                    ],
                                    "type": "string"
                                },
                                "obfuscatedId": {
                                    "description": "Obfuscated (privacy-protected) ID of the user, to identify number of\nunique users using the service.",
                                    "format": "int64",
                                    "type": "string"
                                },
                                "originalMediaType": {
                                    "description": "The original media the speech was recorded on.",
                                    "enum": [
                                        "ORIGINAL_MEDIA_TYPE_UNSPECIFIED",
                                        "AUDIO",
                                        "VIDEO"
                                    ],
                                    "type": "string"
                                },
                                "originalMimeType": {
                                    "description": "Mime type of the original audio file.  For example `audio/m4a`,\n`audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`.\nA list of possible audio mime types is maintained at\nhttp://www.iana.org/assignments/media-types/media-types.xhtml#audio",
                                    "type": "string"
                                },
                                "recordingDeviceName": {
                                    "description": "The device used to make the recording.  Examples 'Nexus 5X' or\n'Polycom SoundStation IP 6000' or 'POTS' or 'VoIP' or\n'Cardioid Microphone'.",
                                    "type": "string"
                                },
                                "recordingDeviceType": {
                                    "description": "The type of device the speech was recorded with.",
                                    "enum": [
                                        "RECORDING_DEVICE_TYPE_UNSPECIFIED",
                                        "SMARTPHONE",
                                        "PC",
                                        "PHONE_LINE",
                                        "VEHICLE",
                                        "OTHER_OUTDOOR_DEVICE",
                                        "OTHER_INDOOR_DEVICE"
                                    ],
                                    "type": "string"
                                }
                            },
                            "type": "object"
                        },
                        "model": {
                            "description": "*Optional* Which model to select for the given request. Select the model\nbest suited to your domain to get best results. If a model is not\nexplicitly specified, then we auto-select a model based on the parameters\nin the RecognitionConfig.\n<table>\n  <tr>\n    <td><b>Model</b></td>\n    <td><b>Description</b></td>\n  </tr>\n  <tr>\n    <td><code>command_and_search</code></td>\n    <td>Best for short queries such as voice commands or voice search.</td>\n  </tr>\n  <tr>\n    <td><code>phone_call</code></td>\n    <td>Best for audio that originated from a phone call (typically\n    recorded at an 8khz sampling rate).</td>\n  </tr>\n  <tr>\n    <td><code>video</code></td>\n    <td>Best for audio that originated from from video or includes multiple\n        speakers. Ideally the audio is recorded at a 16khz or greater\n        sampling rate. This is a premium model that costs more than the\n        standard rate.</td>\n  </tr>\n  <tr>\n    <td><code>default</code></td>\n    <td>Best for audio that is not one of the specific audio models.\n        For example, long-form audio. Ideally the audio is high-fidelity,\n        recorded at a 16khz or greater sampling rate.</td>\n  </tr>\n</table>",
                            "type": "string"
                        },
                        "profanityFilter": {
                            "description": "*Optional* If set to `true`, the server will attempt to filter out\nprofanities, replacing all but the initial character in each filtered word\nwith asterisks, e.g. \"f***\". If set to `false` or omitted, profanities\nwon't be filtered out.",
                            "type": "boolean"
                        },
                        "sampleRateHertz": {
                            "description": "Sample rate in Hertz of the audio data sent in all\n`RecognitionAudio` messages. Valid values are: 8000-48000.\n16000 is optimal. For best results, set the sampling rate of the audio\nsource to 16000 Hz. If that's not possible, use the native sample rate of\nthe audio source (instead of re-sampling).\nThis field is optional for `FLAC`,  `WAV`. and 'MP3' audio files, and is\nrequired for all other audio formats. For details, see AudioEncoding.",
                            "format": "int32",
                            "type": "integer"
                        },
                        "speechContexts": {
                            "description": "*Optional* array of SpeechContext.\nA means to provide context to assist the speech recognition. For more\ninformation, see [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).",
                            "items": {
                                "description": "Provides \"hints\" to the speech recognizer to favor specific words and phrases\nin the results.",
                                "properties": {
                                    "phrases": {
                                        "description": "*Optional* A list of strings containing words and phrases \"hints\" so that\nthe speech recognition is more likely to recognize them. This can be used\nto improve the accuracy for specific words and phrases, for example, if\nspecific commands are typically spoken by the user. This can also be used\nto add additional words to the vocabulary of the recognizer. See\n[usage limits](/speech-to-text/quotas#content).",
                                        "items": {
                                            "type": "string"
                                        },
                                        "type": "array"
                                    }
                                },
                                "type": "object"
                            },
                            "type": "array"
                        },
                        "useEnhanced": {
                            "description": "*Optional* Set to true to use an enhanced model for speech recognition.\nIf `use_enhanced` is set to true and the `model` field is not set, then\nan appropriate enhanced model is chosen if:\n1. project is eligible for requesting enhanced models\n2. an enhanced model exists for the audio\n\nIf `use_enhanced` is true and an enhanced version of the specified model\ndoes not exist, then the speech is recognized using the standard version\nof the specified model.\n\nEnhanced speech models require that you opt-in to data logging using\ninstructions in the\n[documentation](/speech-to-text/docs/enable-data-logging). If you set\n`use_enhanced` to true and you have not enabled audio logging, then you\nwill receive an error.",
                            "type": "boolean"
                        }
                    },
                    "type": "object"
                }
            },
            "type": "object"
        },
        "__xgafv": {
            "type": "string",
            "enum": [
                "1",
                "2"
            ]
        }
    }
}